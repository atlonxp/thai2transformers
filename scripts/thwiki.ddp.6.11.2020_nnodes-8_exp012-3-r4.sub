#!/bin/sh
#SBATCH --output=./slurm_output/exp012-3-r4_thwiki-ddp_6.11.2020_run-on-17.11.2020.job-%j.node-%n.out
#SBATCH --nodes=8
#SBATCH --partition=gpu-cluster
#SBATCH --account=scads
#SBATCH --time=1-15:0:0
#SBATCH --gres=gpu:4
#SBATCH --mem=100GB
#SBATCH --job-name=exp012-3-r4_thwiki-ddp_6.11.2020_nnodes-8_run-on-17.11.2020

module load CUDA/10.2

unset XDG_RUNTIME_DIR
if [ "$SLURM_JOBTMP" != "" ]; then
     export XDG_RUNTIME_DIR=$SLURM_JOBTMP
fi

HOSTNAME=$(hostname -s)
JOBID=${SLURM_JOB_ID}
N_PROC_PER_NODE=4

N_NODES=8
MAX_STEPS=31250
WARMUP_STEPS=1250
SAVE_STEPS=3125
EVAL_STEPS=3125
EXP_NAME=exp012-3-r4_thwiki-for-ddp_6.11.2020_spm_vs-24k_fp16_bz16_maxstep-31250_ngpus-32_maxseqlen-512_linebyline
LR=7e-4
BATCH_SIZE=16
GRAD_ACC=16
MODEL_CHECKPOINT_DIR=none
BLOCK_SIZE=514


srun ./thwiki-ddp-6.11.2020_train-mlm-th-roberta.sh $N_NODES $HOSTNAME $N_PROC_PER_NODE $JOBID \
$MAX_STEPS $WARMUP_STEPS $SAVE_STEPS $EVAL_STEPS \
$EXP_NAME \
$LR $BATCH_SIZE $GRAD_ACC \
$MODEL_CHECKPOINT_DIR \
$BLOCK_SIZE

